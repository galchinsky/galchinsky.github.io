---
layout : post
title : "Знай свои данные: деблюр"
---

Пост, посвященный методам генерации данных в задачах image super-resolution, deblur и тому подобных. Рассмотрен взгляд с точки зрения оптики. Полезно может быть также тем, кто знанимается CV, но в оптике есть какие-то пробелы. Я постарался срезать углы, чтобы не зарываться в детали, хотя местами все равно зарылся.

## Основы оптики

В этом параграфе описано, почему сложно сделать идеальное изображение, и какие проблемы есть с тем, что дают нам объективы из реального мира.

Обычно свет распространяется от видимого предмета сразу во все стороны. Поэтому, если взять белый лист бумаги, на нем не будет ничего отображаться - лучи падают на этот лист со всех сторон, перемешиваясь. Фильмы про хакеров врут: такое, как на картинке, когда на лице отпечатываются полоски текста, невозможно.

![Кадр из матрицы, в котором на актера спроецирован монитор](https://user-images.githubusercontent.com/2237541/143165644-24467b0d-c2ff-42d6-a551-d287f7041fd5.png)

Чтобы увидеть на белом листе предметы, нужна штука, которая вычленит лучи от какой-то конкретной точки из этой электромагнитной каши. Простейший способ - небольшая дырка. Из всех лучей, падающих на нас, отсекаем небольшую область. Ставим на пути лучей плоскость. Получаем изображение. Чем меньше дырка, чем четче изображение.

![Изображение камеры-обскуры из википедии](https://user-images.githubusercontent.com/2237541/143166077-85014819-0fc9-4f28-9a67-aba49bd92c7c.png)

Но возникают трудности - световой энергии проходит мало. Чем меньше дырка, тем тусклее. Поэтому в реальности - это всегда дневной пейзаж.

В компьютерной графике, где об энергии можно не заботиться, такой способ построения изображений стал основным. Это удобно, потому что геометрическая задача несложная: провести прямую через 2 точки и найти пересечение с плоскостью. Для пущей простоты в компьютерах меняют местами плоскость и дырку. Физически это уже неосуществимо, зато изображение не переворачивается.

![Проекция в 3D графике](https://user-images.githubusercontent.com/2237541/143167935-7aaec213-c906-466f-a69e-55facf5c8487.png)

В реальном же мире используют линзы. Линза собирает лучи с большей области пространства, и направляет их примерно в одну точку. Получается намного ярче, но возникают новые проблемы. С дыркой у нас все было настолько четко, насколько эта дырка маленькая (волновые свойства света в расчет не берем). С линзой все начинает зависеть от того, где расположен лист, но даже если он расположен в наилучшем месте, все равно где-то будет более размыто, где-то менее. У дырки прямые линии остаются прямыми. Линза будет их гнуть.

Таким образом, с одной стороны у нас есть идеальное изображение, которое создают 3D-движки, а также несуществующая камера с бесконечно малой дыркой, которая, несмотря на свое несуществование, является полезной ментальной моделью. С другой стороны, у нас есть реальное изображение, построенное с помощью линз. Чем идеальное отличается от реального?

### Дефокусировка

Проблема нулевая - когда у нас простая дырка, перемещение плоскости меняет в основном размер изображения. Линзе же не все равно, на каком расстоянии предмет. Если он бесконечно далеко, как звезда, а лучи падают почти параллельно - плоскость должна быть в одном месте. Если предмет близко - то в другом. Необходим способ *фокусироваться* - менять на лету расстояние между линзой и плоскостью, подстраиваясь под расстояние до предмета. Также появляется понятия глубины резкости - некоторые линзы изображают четко довольно большие области, с некоторыми в фокус еще попасть нужно. Так как объективы в наше время делать более-менее умеют, дефокусировка - главная проблема неудачных кадров.

<!-- ![how-focus-works](https://user-images.githubusercontent.com/2237541/143169296-35572fa1-35ae-451a-bd4e-90ff7314a417.gif =250x) -->
<img src="https://user-images.githubusercontent.com/2237541/143169296-35572fa1-35ae-451a-bd4e-90ff7314a417.gif" alt="Как работает фокус, (c) Qt. Изображен ход лучей у линзы при разном положении плоскости проекции" width="350">

### Сферическая аберрация

Проблема первая - технологически проще всего делать плоские и сферические поверхности. Берешь шар, трешь об него стекло как угодно - стекло примет форму вогнутого шара. То же самое с выпуклым шаром и плоскостью. С другой поверхностью так не получится, потому что в разных местах будет разная кривизна. Но геометрическая фигура, которая сводит лучи идеально в точку - параболоид.

Параболоид сделать тяжелее. Компромисс - найти такую сферу, чтобы она максимально повторяла параболоид. Идеально такую линзу сфокусировать нельзя, в минимальном случае будет небольшое пятно. Это пятно называется сферической аберрацией. 

![Разница в ходе лучей у сферической и параболических линз](https://user-images.githubusercontent.com/2237541/143169746-20c1d3e4-9e6b-42f2-babe-0aba38048f73.png)

Если в оптической системе используется что-то, кроме сфер, это называется асферической оптикой. Сейчас асферикой никого не удивишь, но по-прежнему точную сферу сделать намного проще, поэтому количество таких поверхностей стараются минимизировать.

### Хроматизм

Проблема вторая - линза преломляет красные, зеленые и синие лучи слегка по-разному. В итоге, там, где зеленые лучи сведены минимально, красные и синие будут чуть шире. Если передвинуть плоскость, чтобы красные стали четкими - зеленые и синие разъедутся. Это называется хроматической аберрацией. Максимальную четкость зеленого считают основной, потому что так считает глаз. 

![Сверху - разное преломление лучей при хроматизме. Снизу - изображение Луны с радужной каймой](https://user-images.githubusercontent.com/2237541/143170845-fe792296-d0de-4d1b-9c41-372ec4746480.png)

### Кома и астигматизм

Проблема третья - если предмет находится не на оси, лучи перекашивает еще больше, а размытое пятно, похожее на гауссоиду, становится ассиметричным, и похожим то ли на запятую, то ли на комету. Это называется комой.

![Ход лучей при коме](https://user-images.githubusercontent.com/2237541/143171124-be2e568c-21b1-4132-9ad6-6d640d00ba86.png)

Стекла, как правило, имеют осевую симметривую. Поэтому кома смотрит в центр объектива и одна и та же на данном расстоянии от центра.

В этот же пункт следует добавить астигматизм. Если предмет размывается не в круг, а эллипс - это он. Объединить их следует из-за того, что в размытии здесь возникает какая-то асимметрия. Из-за осевой симметрии этот эллипс устроен также: один и тот же на данном расстоянии от центра.

### Дистросия

Почему у гитары - дисторшен, а в оптике - дисторсен? Не знаю.

![image](https://user-images.githubusercontent.com/2237541/143171278-d52287c3-867f-4338-985c-39623d06ce15.png)

Описать ее можно так: все может быть четко, но прямые линии не прямые. Это называется дисторсией. Не всегда это является недостатком. Например, загнать широкоугольный объектив в конечные размеры кадра без дисторсии не получится.

![Фотография, сделанная на широкоугольный объектив](https://user-images.githubusercontent.com/2237541/143171495-63d30c69-2277-43ee-aebe-9c9d918b9ae1.png)

Любой, кто устанавливал FOV 170 в шутерах, знает, как выглядит широкоугольное изображение без дисторсии:

![image](https://user-images.githubusercontent.com/2237541/143173176-63a39838-7444-4444-b4f5-44a9577b0d14.png)

Хорошие новости в том, что дисторсия сама по себе не размывает. Плохие - в том, что искажает линии, и если вы деблюрите ширик, возможно следует вносить подобные искажения в фотографии из интернета.

### Моушен блюр

Стекла здесь не при чем, а при чем то, что фотоприемник фиксирует свет в течение какого-то времени, за которое предмет мог сдвинуться.

![image](https://user-images.githubusercontent.com/2237541/143173999-b224d8ef-a536-4920-af60-3ae287aba0eb.png)

### Растекание заряда

Для съемки чего-то тусклого фотоприемнику нужно долго фиксировать свет и считать число попавших в него фотонов. Но если одновременно с этим в кадре есть что-то яркое, электрический заряд в пикселе фотоприемника может начать утекать. То есть, яркий предмет будет белым, а по его границе возникнет дымка.

## Мутный мрак

Задача деблюра - из мутного изображения сделать четкое. Значит нужно научиться из четкого изображения делать мутное, и обучить нейронку делать наоборот.

Четкое изображение получить несложно: скачать картинку 4096х4096, убедиться, что она в фокусе, и ресайзнуть до 512х512 [идеальным алгоритмом без алиасинга](https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing). Даже если объектив имел какие-то недостатки, при таком ресайзе они пропадут. Важный момент: аугментации типа поворота должны делаться до ресайза, размывать тоже желательно в максимально хорошем качестве картинки, просто на всякий случай.

Рецептов, как убедиться, что картинка четкая, [в интернете много](https://stackoverflow.com/questions/7765810/is-there-a-way-to-detect-if-an-image-is-blurry). Они сводятся к тому, что в четком изображении много высоких частот, а значит можно сделать фильтр высоких частот, который посчитает их относительное количество и оценит четкость.

Поэтому, качаем большое и качественное изборажение, режем его на кропы, убеждаемся, что в кропах нет размытых частей. Вроде ничего не забыл? Сделать четкое изображение мутным сложнее, чему будет посвящен остаток поста.

### Моделирование с помощью PSF

В мире с бесконечными вычислительными ресурсами мы могли бы максимально имитировать природу. Поместил виртуально распечатанное изображение на каком-то расстоянии от линзы. Пустил из предмета луч, он летит, пока не столкнется с чем-то, если это что-то - линза, то много раз преломится по формуле преломления, и попадет (или нет) в фотоприемник. Там он пройдет через RGB-сетку и оцифруется вместе с электрическими шумами фотоприемника.

Но ресурсы конечные, надо упростить. Первое, что можно заметить, каждый пиксель можно рассматривать независимо, ведь световые лучи друг с другом не взаимодействуют. Поэтому каждая точка на предмете превратится в какую-то размытую область, а итоговое изображение будет суммой таких размытых областей. Поэтому, нам достаточно посчитать один раз, как пиксель, светящий из данной области пространства заблюрится в результате прохода через линзу. А затем как-то применить это знание.

Это называется PSF - "функция рассеяния точки". Такое моделирование чем-то это напоминает [свертку](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html), но ядро переменное, зависящие от пикселя.

```
for x_изображения
 for y_изображения    
  кернел = PSF(x, y, расстояние)
  for x_кернела        
   for y_кернела        
       результат[y_изображения+y_кернела, x_изображения+x_кернела] += изображение[y_изображения, x_изображения] * кернел[y_кернела, x_кернела]
```

PSF считается один раз, поэтому можно посчитать и трассировкой лучей, как в случае решения влоб. Киношные трассировщики вряд ли подойдут. Подойдут те, что встроены в оптические пакеты. Есть платные, типа Zemax. Есть бесплатные, типа GNU Optical. Как-то я его форкнул, добавив CMake, а потом его форкнул 
[Dibyendu Majumdar](https://github.com/dibyendumajumdar/goptical).

Можно вообще ничего не вычислять, а PSF'ы нарисовать. Но как их нарисовать, если никогда их не видел? Если есть доступ к RAW файлам с фотоприемника, создаем большой черный файл с сеткой из белых точек. Затем в черной комнате с включенным монитором фотографируем его на расстоянии. Важно, чтобы пиксель монитора был меньше пикселя фотоприемника. Если фокус автоматический, нужно поместить рядом какой-нибудь светящийся предмет, чтобы автоматика сфокусировалась. Дальше анализируем, каким образом лучше всего замоделировать получившуюся картинку. Ниже, в разделе про кому, я предлагаю моделировать PSF как сумму гауссоид.

### Размытие по Гауссу

Если PSF все время постоянная, модель сводится с свертке.

```
for x_изображения    
 for y_изображения    
  for x_кернела        
   for y_кернела        
       результат[y_изображения+y_кернела, x_изображения+x_кернела] += изображение[y_изображения, x_изображения] * кернел[y_кернела, x_кернела]
```

Встречается ли такое в реальности? Ну, при сильном расфокусе, наверное, если объектив неплохой. При этом PSF при расфокусе похожа на гауссоиду. Вот мы и придумали guassian blur из фотошопа. Идеальный алгоритм для старых медленных компьютеров, но основую суть выхватывает, будучи очень простым.

```
filtered_image = scipy.ndimage.gaussian_filter(input, sigma)
```

Внутри там что-то типа

```
kernel = gkern(kernel_size, sigma) # описана ниже
filtered_image = np.convolve2d(image, kernel)
```

Единственный параметр тут - sigma, управляет силой размытия. Можно подогнать sigma под величину PSF вручную. Второй параметр - kernel_size, больше касается скорости обработки. Если его поставить слишком маленьким, пятно станет таким толстым, что вытечет из экрана. 

Так как 99,7% энергии попадает в круг +-3 sigma, примерно такого размера kernel_size и нужно делать.

### Другие случаи постоянной PSF

Также можно считать PSF постоянной, если в пределах receptive field выходного пикселя она слабо меняется.

### Хроматическая аберрация

- имитируется разными PSF для разных каналов. В случае размытия по Гауссу:

```python
filtered_image = np.zeros_like(input)
filtered_image[:, :, 0] = scipy.ndimage.gaussian_filter(input[:, :, 0], sigma_r)
filtered_image[:, :, 1] = scipy.ndimage.gaussian_filter(input[:, :, 1], sigma_g)
filtered_image[:, :, 2] = scipy.ndimage.gaussian_filter(input[:, :, 2], sigma_b)
```

sigma_b обычно минимальная, потому что объективы фокусируются по зеленому каналу.

### Кома

Если мы обучаем на мелких кропах, PSF можно считать постоянной в пределах кропа. Но использовать только гауссово размытие неправильно, потому что оно соответствует идеально симметричной PSF. Ближе к краю кадра эта симметрия, как правило, нарушается. Как сгенерировать PSF  с перекосом? Можно делать это с помощью накладывания множества гауссоид в цикле. Алгоритму требуется начальная, конечная точка и то, как меняется ширина пятна. Диапазоны для рандомизатора можно установить, внимательно изучив PSF реальных объективов. Так можно объективы с комой, астигматизмом, моделировать реальные PSF, меняя только xs и ys:

```python
mport matplotlib.pyplot as plt
import numpy as np
import scipy.ndimage

kernel_size = 128
steps = 100
ts = np.linspace(0, 1, steps)
xs = np.interp(ts, [0, 1], [0.35, 0.55]) * kernel_size
ys = np.interp(ts, [0, 1], [0.5, 0.8]) * kernel_size
sigmas = np.interp(ts, [0, 1], [0.05, 0.1]) * kernel_size

def gkern(kernel_size, x, y, sigma):
    x_axis = np.arange(-x, kernel_size - x)
    y_axis = np.arange(-y, kernel_size - y)
    xx, yy = np.meshgrid(x_axis, y_axis)
    kernel = np.exp(-0.5 * (np.square(xx) + np.square(yy)) / np.square(sigma))
    return kernel

a = np.zeros((kernel_size, kernel_size))
for x, y, sigma in zip(xs, ys, sigmas):
    a = a + gkern(kernel_size, x, y, sigma)

# normalize in two steps, first energy value
a = a/np.sum(a)
# second: maxmimum energy should be in the center of kernel
x, y = scipy.ndimage.measurements.center_of_mass(a)
a = scipy.ndimage.interpolation.shift(a, [kernel_size//2 - x, kernel_size//2 - y])

plt.imshow(a)
plt.show()
```

### Моушен блюр

Чтобы добавить мошуен блюр, нужно свернуть PSF с прямой линией.

### Сколько PSF нужно

Объективы осесимметричны. То есть если в точке (5; 0) мы имеем PSF, то в точке (0; 5) будет такая же картина, но повернутая на 90 градусов. Это позволяет ввести функцию PSF(r), посчитав PSF только вдоль одной линии от центра к краю. Для остальных мест нужно найти r=sqrt(x^2+y^2) и просто повернуть изображение на atan2(y, x). Когда разговор заходит о поворотах и ресайзах, нужно убедиться, что у исходного изображения достаточно разрешения. Поэтому посчитать PSF сеткой по x и y тоже подход неплохой. Тогда в узловых точках мы вычисляем PSF(x, y), между ними - интерполируем между 4 ближайшими PSF. Все зависит от того, насколько дорого считать PSF. Когда я пришел на свою первую работу, как это автоматизируется я не знал, и мне давали какие-то таблички оптики, которые вручную считали в Zemax. Мне это не понравилось, я взял GNU Optical, чтобы считать это автоматически.

## Ближе к железу

### LocallyConnectedLayer

Существует готовый слой, способный делать свертку с переменным ядром. Можно загрузить в него нужные веса, и размывать на GPU. Он занимает много памяти, потому что хранит кернел для каждого пикселя. Но зато PSF для каждого пикселя вычисляется всего один раз.

### Быстрая свертка с PSF в узловых точках

Разбиваем изображение на перекрывающиеся квадраты. Размываем в них по-разному (в зависимости от PSF в данной области кадра). Собираем изображение назад. Чтобы избежать краевых эффектов, блендим квадраты плавно между собой, используя что-то типа hann window. Таким образом получаем дешевую свертку с переменным ядром.

![hann window overlap](https://user-images.githubusercontent.com/2237541/143183047-bb598094-7a90-49ae-b962-99ad68448f75.png)

### Разные расстояния

Две оптимизации выше работают, только если в PSF(x, y, расстояние) расстояние постоянно. То же касается и LocallyConnectedLayer. То есть, физически это значит, что мы распечатали картинку, поставили ее на каком-то расстоянии от объектива, и что-то получили.

Это очень сильное предположение, потому что зачастую нам интересен как раз контраст фигур на фоне. Возможно, мы не хотим деблюрить фон вообще, только передний план.

Возможен следующий трюк: разбиваем изображение на серию плоских изображений с прозрачностью, соответствую переднему и заднему планам. К переднему плану применяем PSF, как будто все в фокусе. К заднему плану - другую, более размытую PSF. Затем складываем два изображения.

Разбить изображение можно уже натренированной нейросетью.


